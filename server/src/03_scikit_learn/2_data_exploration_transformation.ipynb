{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b648ee8",
   "metadata": {},
   "source": [
    "# NOTEBOOK 2: Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed15903",
   "metadata": {},
   "source": [
    "## Imports libraries for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import version\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities.creds import Credentials\n",
    "from snowflake.snowpark import version\n",
    "print(version.VERSION)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1f95920",
   "metadata": {},
   "source": [
    "## Create Snowpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49090e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(Credentials().__dict__).create()\n",
    "\n",
    "session.use_role(\"LEARNINGSNOWPARKROLE\")\n",
    "session.use_database(\"SCIKIT_LEARN\")\n",
    "session.use_schema(\"SCIKIT_LEARN.PUBLIC\")\n",
    "session.use_warehouse(\"LEARNINGSNOWPARKVW\")\n",
    "\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1c0c26",
   "metadata": {},
   "source": [
    "## Pandas DataFrames compared to Snowpark DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f84673f",
   "metadata": {},
   "source": [
    "This section showcase the differences between using a Pandas Dataframe (data in memory on client machine) and a Snowpark DataFrame (data in Snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Pandas DataFrame\n",
    "pandas_df = pd.read_csv('datasets/housing/housing.csv')\n",
    "print(type(pandas_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623695f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Snowpark DataFrame\n",
    "snowpark_df = session.table('HOUSING_DATA')\n",
    "print(type(snowpark_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d45181",
   "metadata": {},
   "source": [
    "Since a Snowpark DataFrame does not contain any dayta, ie only a \"pointer\" to data in Snowflake, the memory used by it on the client side is minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare size\n",
    "print('Size in MB of Pandas DataFrame in Memory:\\n', np.round(sys.getsizeof(pandas_df) / (1024.0**2), 2))\n",
    "print('Size in MB of Snowpark DataFrame in Memory:\\n', np.round(sys.getsizeof(snowpark_df) / (1024.0**2), 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22eefbc4",
   "metadata": {},
   "source": [
    "A Snowpark DataFrame can be easily converted to a Pandas DataFrame by using the **to_pandas** method, this will cause the data to be pulled back from Snowflake and loaded into the client memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a Snowpark DataFrame to Pandas DataFrame\n",
    "pandas_df_from_snowflake = snowpark_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.shape, pandas_df_from_snowflake.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d275bde",
   "metadata": {},
   "source": [
    "To have a peak of the data that a Snowpark DataFrame is representing **show** function can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aea119a6",
   "metadata": {},
   "source": [
    "Looking at the queries will show us what is actual keept in the client memmory, the SQL needed to return the data accordingly to our DataFram defenition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc67044f",
   "metadata": {},
   "source": [
    "The Snowpark DataFrame API supports multiple ways to select specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a577f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "snowpark_df_subset = snowpark_df.select('HOUSING_MEDIAN_AGE','TOTAL_ROOMS','TOTAL_BEDROOMS','HOUSEHOLDS','OCEAN_PROXIMITY')\n",
    "snowpark_df_subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-like syntax for column selection from Snowflake dataframe\n",
    "snowpark_df_subset = snowpark_df[['HOUSING_MEDIAN_AGE','TOTAL_ROOMS','TOTAL_BEDROOMS','HOUSEHOLDS','OCEAN_PROXIMITY']]\n",
    "snowpark_df_subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49579412",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_subset.queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01b2bba5",
   "metadata": {},
   "source": [
    "**with_column** function can be used to add a new column to a Snowpark DataFrame (with_columns allows us to add multiple at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_new_col = snowpark_df_subset.with_column('BEDROOM_RATIO', F.col('TOTAL_BEDROOMS') / F.col('TOTAL_ROOMS'))\n",
    "snowpark_df_new_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_new_col.queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c86b505d",
   "metadata": {},
   "source": [
    "To remove a column from a Snowpark DataFrame **drop** function can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6057de",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_drop_col = snowpark_df_new_col.drop('BEDROOM_RATIO')\n",
    "snowpark_df_drop_col.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73d1420a",
   "metadata": {},
   "source": [
    "To filter (select rows) from a Snowpark DataFrame **filter** or **where** can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_filtered = snowpark_df_drop_col.filter(F.col('OCEAN_PROXIMITY').in_(['INLAND','ISLAND', 'NEAR BAY']))\n",
    "snowpark_df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_filtered.queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e769489",
   "metadata": {},
   "source": [
    "To to aggregation of the data in a Snowpark DataFrame **group_by** and **agg** can used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "snowpark_df_agg = snowpark_df_filtered.group_by(['OCEAN_PROXIMITY']).agg([F.avg('HOUSEHOLDS').as_('AVG_HOUSEHOLDS')])\n",
    "snowpark_df_agg.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45dbda51",
   "metadata": {},
   "source": [
    "The returned result for a Snowpark DataFrame can be sorted using **sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df_sorted = snowpark_df_agg.sort(F.col('AVG_HOUSEHOLDS').asc())\n",
    "snowpark_df_sorted.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c5138d1",
   "metadata": {},
   "source": [
    "## Data Preprocessing using Scikit\n",
    "\n",
    "Let's start by getting some basic understanding of our data.\n",
    "\n",
    "We can use the **describe** function on our **numeric** and **character** columns to get some basic statistics, count shows number of non null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25de7ebf",
   "metadata": {},
   "source": [
    "Above shows that TOTAL_BEDROOMS has missing values ie the count is less than 20640, so we need to manage that before training a model.\n",
    "\n",
    "Using the schema of a Snowpark DataFrame allows us to easily get the numerical and categorical (character) column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c475dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all numerical columns\n",
    "numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n",
    "numeric_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in numeric_types]\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all categorical columns (columns with character data type)\n",
    "categorical_types = [T.StringType]\n",
    "categorical_columns = [c.name for c in snowpark_df.schema.fields if type(c.datatype) in categorical_types]\n",
    "categorical_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6b9601d",
   "metadata": {},
   "source": [
    "Now we will impute missing values from total_bedroom using scikit learn impute function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pull back the data from SNowflake into a Pandas Dataframe, data now is stored in memory\n",
    "pandas_df = snowpark_df.to_pandas()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "imputer = imputer.fit(pandas_df[['TOTAL_BEDROOMS']])\n",
    "pandas_df['TOTAL_BEDROOMS'] = imputer.transform(pandas_df[['TOTAL_BEDROOMS']])\n",
    "pandas_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1dcf034",
   "metadata": {},
   "source": [
    "Now if we print count of total_bedrooms column we will see full count of 20640."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_df[\"TOTAL_BEDROOMS\"].count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "524a54d2",
   "metadata": {},
   "source": [
    "Similarly machine learning models expects data to be normalised before training the models.\n",
    "\n",
    "For that we can use scikit learn normalise functions to normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9798da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df_norm = preprocessing.normalize(pandas_df[[\"LATITUDE\",\"LONGITUDE\",\"TOTAL_BEDROOMS\"]].dropna())\n",
    "df_norm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc19b6c9",
   "metadata": {},
   "source": [
    "What we did so far with scikit learn functions was using pandas dataframe which was all executed on local machine, but in next worksheet we will see how we can run all this inside Snowflake."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d219728c",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "\n",
    "To understand which features are useful for our machine learning models we can do some visualisation on our data set to get better view of our data. \n",
    "Let's create a basic visualisation on data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by creating a pie chart. To create pie chart we will OCEAN_PROXIMITY column and see its distribution.\n",
    "# First we get the distinct values in column OCEAN_PROXIMITY and the number of rows for each unique value\n",
    "# We are are using pyplot for this visualisation\n",
    "\n",
    "df_pie = snowpark_df.group_by(\"OCEAN_PROXIMITY\").agg(F.sum('MEDIAN_HOUSE_VALUE').as_('MEDIAN_HOUSE_VALUE')).to_pandas()\n",
    "df_pie.set_index('OCEAN_PROXIMITY', inplace=True)\n",
    "df_pie.plot.pie(y='MEDIAN_HOUSE_VALUE', figsize=(8,8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "954f2b38",
   "metadata": {},
   "source": [
    "To analyse distribution of our continous variables we will plot histograms for all continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for all continous variables\n",
    "\n",
    "pd_numeric = snowpark_df.select(numeric_columns).to_pandas()\n",
    "pd_numeric.hist(bins=30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8000f0",
   "metadata": {},
   "source": [
    "Plotting correlation matrix helps to identify how different features are related to each others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723420e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use seaborn lib to plot correlation matrix\n",
    "sn.heatmap(snowpark_df.to_pandas().corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
