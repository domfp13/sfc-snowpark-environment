{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK 3: END TO END ML USING SNOWPARK AND SCIKIT-LEARN\n",
    "\n",
    "In this notebook we fit/train a Scikit-Learn ML pipeline that includes common feature engineering tasks such as Imputations, Scaling and One-Hot Encoding. The pipeline also includes a `RandomForestRegressor` model that will predict median house values in California. \n",
    "\n",
    "We will fit/train the pipeline using a Snowpark Python Stored Procedure (SPROC) and then save the pipeline to a Snowflake stage. This example concludes by showing how a saved model/pipeline can be loaded and run in a scalable fashion on a snowflake warehouse using Snowpark Python User-Defined Functions (UDFs). \n",
    "\n",
    "We will also use Snowpark Optimized warehouse in this notebook.\n",
    "\n",
    "![Snowpark ML](images/snowflake_e2e_ml.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a session with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark libs\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import version\n",
    "\n",
    "# Sickit-learn libs\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import cachetools\n",
    "\n",
    "#Snowflake connection info\n",
    "from config import snowflake_conn_prop\n",
    "\n",
    "print(f'Snowpark version : {version.VERSION}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities.creds import Credentials\n",
    "session = Session.builder.configs(Credentials().__dict__).create()\n",
    "\n",
    "# Setting up the session environment\n",
    "session.use_role(\"LEARNINGSNOWPARKROLE\")\n",
    "session.use_database(\"SCIKIT_LEARN\")\n",
    "session.use_schema(\"SCIKIT_LEARN.PUBLIC\")\n",
    "session.use_warehouse(\"LEARNINGSNOWPARKVW\")\n",
    "\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Snowflake stage to save the ML model/pipeline and permanent UDFs\n",
    "\n",
    "In order to create a permanent Stored Procedure, model training in Snowflake, UDF to score the model in Snowflake, and store the model file we need a Snowflake stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_name = 'qs_sklearn_stage'\n",
    "# collect function triggers execution of the SQL\n",
    "session.sql(f\"create or replace stage {stage_name}\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stored Proc fits the pipeline and the model and then saves it in Snowflake\n",
    "\n",
    "Start by creating a training function, that creates a pipline with preprocessing of the data and then train a RandomForestRegressor model.\n",
    "\n",
    "We already saw some preprocessing steps in previous notebook but now we will create it as a function which will then be packaged as Stored procedure to run this entire python function in Snowflake\n",
    "\n",
    "We will use scickit-learn for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def fit_pipeline(X, y, cat_attribs, num_attribs):\n",
    "\n",
    "    # create a pipeline for numerical features\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    # Pipeline for categorical features\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Create the preprocessor\n",
    "    preprocessor = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", cat_pipeline, cat_attribs)\n",
    "        ])\n",
    "\n",
    "    # Create the full pipeline wincluding the model training\n",
    "    full_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "    # fit the preprocessing pipeline and the model together\n",
    "    full_pipeline.fit(X, y)\n",
    "\n",
    "    return full_pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the training function locally we will ned to pull back the data into a Pandas DataFrame, by using the **sample** method we can get 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = session.table(\"HOUSING_DATA\").sample(frac=0.10).to_pandas()\n",
    "pd_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd_test.loc[:, pd_test.columns != 'MEDIAN_HOUSE_VALUE']\n",
    "y = pd_test['MEDIAN_HOUSE_VALUE']\n",
    "\n",
    "test_full_pipe = fit_pipeline(X, y,  ['OCEAN_PROXIMITY'], ['LONGITUDE', 'LATITUDE', 'HOUSING_MEDIAN_AGE', 'TOTAL_ROOMS',\n",
    "       'TOTAL_BEDROOMS', 'POPULATION', 'HOUSEHOLDS', 'MEDIAN_INCOME'])\n",
    "test_full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first prediction\n",
    "test_full_pipe.predict(X)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now deploy the training function as a Python Stored Procedure in Snowflake, so we can run the training on Snowflake compute and do not need to move data around.\n",
    "\n",
    "We also want to save the trained model (pipeline) as a file so we can use it in UDF, scoring function, later. The palce to save it is in a Snowflake stage and we can create a function to do that, in real life we would already have that function in a utility model we can resue for multiple projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a Python object to a Snowflake internal stage\n",
    "def save_file(snf_session, object, stage_name, stage_path, file_name):\n",
    "  import io\n",
    "  import joblib\n",
    "\n",
    "  save_path = stage_name + '/' + stage_path\n",
    "  input_stream = io.BytesIO()\n",
    "  input_stream.name = file_name\n",
    "  joblib.dump(object, input_stream)\n",
    "  put_result = snf_session.file.put_stream(input_stream, save_path, overwrite=True)\n",
    "  \n",
    "  return f'{save_path}/{put_result.target}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a wrapper function for our training function where we can get the data and convert it to a Pandas DataFrame to be used with the training function, this is the function that will be the logic of the Stored Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored Procedure function\n",
    "def train_model(snf_session: Session # A stored procedure will recive a session object when executed in snowflake with the authentification done\n",
    "                , training_table: str # Table name that has the data to be used for training and test\n",
    "                , target_col: str # name of the target column\n",
    "                , save_stage: str # name of the stage to save the fitted pipline object\n",
    "                ) -> dict: # \n",
    "    \n",
    "    # Libraries used in the function that has not been imported as part of the python session\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    now = datetime.now() # Get the date and time when this is strated\n",
    "    \n",
    "    # Get the training table and split into a training and test Snowpark DataFrames\n",
    "    snowdf_train, snowdf_test = snf_session.table(training_table).random_split([0.8, 0.2], seed=82) # use seed to make the split repeatable\n",
    "\n",
    "    # Get the categorical and numeric column names\n",
    "    cat_attribs = [c.name for c in snowdf_train.schema.fields if (type(c.datatype) == T.StringType) & (c.name != target_col)]\n",
    "    numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n",
    "    num_attribs = [c.name for c in snowdf_train.schema.fields if (type(c.datatype) in numeric_types) & (c.name != target_col)]\n",
    "\n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    table_suffix = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    train_table_name = training_table + '_TRAIN_' + table_suffix\n",
    "    test_table_name = training_table + '_TEST_' + table_suffix\n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(train_table_name)\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(test_table_name)\n",
    "\n",
    "    pd_train = snowdf_train.to_pandas()\n",
    "    \n",
    "    X_train = pd_train.loc[:, pd_train.columns != target_col]\n",
    "    y_train = pd_train[target_col]\n",
    "    \n",
    "    # Fit the model (pipeline)\n",
    "    full_pipeline = fit_pipeline(X_train, y_train, cat_attribs, num_attribs)\n",
    "\n",
    "\n",
    "    # save the full pipeline including the model\n",
    "    \n",
    "    # Save the model to stage\n",
    "    save_path = now.strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "    object_saved_path = save_file(snf_session, full_pipeline, f\"@{save_stage}/models\", save_path, 'housing_fores_reg.joblib')\n",
    "\n",
    "\n",
    "    # predict on the test set and return the root mean squared error (RMSE)\n",
    "    pd_test = snowdf_test.to_pandas()\n",
    "    \n",
    "    X_test = pd_test.loc[:, pd_train.columns != target_col]\n",
    "    y_test = pd_test[target_col]\n",
    "    \n",
    "    housing_predictions = full_pipeline.predict(X_test)\n",
    "    lin_mse = mean_squared_error(y_test, housing_predictions)\n",
    "\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "    # Create a dict to return with test metrics and the path to the saved model pipeline\n",
    "    ret_dict = {\n",
    "        \"MSE\": lin_mse\n",
    "        ,\"RMSE\": lin_rmse\n",
    "        , \"model_path\": object_saved_path\n",
    "        , \"train_table\": train_table_name\n",
    "        , \"test_table\": test_table_name\n",
    "    }\n",
    "    return ret_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the train_model function to Snowflake as a Python stored procedure, Snowpark will also include the fit_pipeline and save_file functions.\n",
    "\n",
    "When deploying a stored procedure we will also need to sepcify what third-party Python libraies the functions are depended on, these libraries must be avalible in the Snowflake Anaconda channel. By using **clear_packages** and **clear_imports** first we make sure that we only include the ones needed for this stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')\n",
    "train_model_sp = F.sproc(func=train_model, name=\"train_house_sp\" ,replace=True, is_permanent=True, stage_location=f\"{stage_name}/sp/\", session=session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training within the SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict = json.loads(train_model_sp(session, \"HOUSING_DATA\", \"MEDIAN_HOUSE_VALUE\", stage_name))\n",
    "return_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally : For use cases where training data size is big you can optimize execution speed of model training by using Snowpark optimized warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a snowpark optimised warehouse\n",
    "#session.sql(\"create or replace warehouse LAB_SCIKIT_SNOWPARK_WH with \\\n",
    "#                WAREHOUSE_SIZE = MEDIUM \\\n",
    "#                AUTO_SUSPEND = 60 \\\n",
    "#                WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED' \\\n",
    "#                AUTO_RESUME = TRUE\").collect()\n",
    "#session.use_warehouse(\"LAB_SCIKIT_SNOWPARK_WH\")\n",
    "# calling the training stored procedure\n",
    "return_dict = json.loads(train_model_sp(session, \"HOUSING_DATA\", \"MEDIAN_HOUSE_VALUE\", stage_name))\n",
    "return_dict\n",
    "# suspending the snowpark optimised warehouse\n",
    "#session.sql(\"ALTER WAREHOUSE LAB_SCIKIT_SNOWPARK_WH SUSPEND\")\n",
    "# using regular warehouse\n",
    "session.use_warehouse(format(snowflake_conn_prop['warehouse']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the model file is stored on the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{stage_name}\").show(max_width=150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/Pipeline Deployment \n",
    "\n",
    "To use the fitted model on new data we can create UDF in Snowflake, that allows us to do the scoring where the data is.\n",
    "\n",
    "Since the model is stored on stage we need to load it as part of the call to the UDF, but we do not wish to read it from stage for every function call so we can then create specific function for loading the file and then use cachetools to cache the result of the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we do not have previous imports and packages added\n",
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import sys\n",
    "       import os\n",
    "       import joblib\n",
    "       # Get the \"path\" of where files added through iport are avalible\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can create the scoring function, predict_house_value, that by using the decorator @F.udf before will be automatically deployed to Snowflake as UDF with the name predict_house_value. By defining the input data type for the function to a PandasDataFrame and the return as PandasSeries, the UDF will run as a vectorized UDF where it will recive a batch of rows for each call.\n",
    "\n",
    "Since we have saved the fitted pipline as a file in stage we need to add it as a import so it can be accessed by the function, through the read_file function created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LONGITUDE', 'LATITUDE', 'HOUSING_MEDIAN_AGE', 'TOTAL_ROOMS',\n",
    "       'TOTAL_BEDROOMS', 'POPULATION', 'HOUSEHOLDS', 'MEDIAN_INCOME', 'OCEAN_PROXIMITY']\n",
    "\n",
    "@F.udf(name=\"predict_house_value\", is_permanent=True, stage_location=f'@{stage_name}/udf/', replace=True\n",
    "              , imports=[return_dict['model_path']]\n",
    "              , packages=['scikit-learn', 'pandas', 'joblib', 'cachetools'])\n",
    "def predict_house_value(pd_df: T.PandasDataFrame[float, float, float, float, float, float, float, float\n",
    "                                                 , str]) -> T.PandasSeries[float]:\n",
    "       pd_df.columns = features\n",
    "       m = read_file('housing_fores_reg.joblib.gz') \n",
    "       return m.predict(pd_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the UDF to make predictions over the test dataset\n",
    "\n",
    "We can now use the UDF to score data that is in Snowflake, we can use the Snowpark DataFrame API for it, but also use SQL or JAVA/SCALA to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to call the udf for inferencing we will use call_function\n",
    "\n",
    "snowdf_test = session.table(\"HOUSING_TEST\")\n",
    "inputs = snowdf_test.drop(\"MEDIAN_HOUSE_VALUE\")\n",
    "                    \n",
    "snowdf_results = snowdf_test.select(*inputs,\n",
    "                    F.call_function(\"predict_house_value\",*inputs).alias('PREDICTION'), \n",
    "                    (F.col('MEDIAN_HOUSE_VALUE')).alias('ACTUAL_LABEL')\n",
    "                    )\n",
    "\n",
    "snowdf_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-de-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "daa716d463a28d756b7a73b3824e39bf40e685223bcb0892720834bc39264f4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
